{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "id": "we2PJZfDi0WU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.distributions import Categorical\n",
        "from torch_geometric.data import Data\n",
        "from itertools import permutations\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch_geometric.nn import GCNConv, LayerNorm, global_add_pool\n",
        "import random\n",
        "\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "class Utils:\n",
        "    @staticmethod\n",
        "    def set_seed(seed):\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        if torch.backends.cudnn.enabled:\n",
        "            torch.backends.cudnn.benchmark = False\n",
        "            torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "class GraphDataProcessor:\n",
        "    @staticmethod\n",
        "    def create_graph_data(data, index_pairs):\n",
        "        edge_index = list(permutations(range(len(data)), 2))\n",
        "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "        node_features = torch.tensor([[data[i], data[j]] for i, j in index_pairs], dtype=torch.float)\n",
        "        return Data(x=node_features, edge_index=edge_index)\n",
        "\n",
        "    @staticmethod\n",
        "    def construct_index_pairs(num_nodes):\n",
        "        index_pairs = [(i, i + 1) for i in range(0, num_nodes - 1, 2)]\n",
        "        index_pairs.extend((i, num_nodes - i - 1) for i in range(num_nodes // 2))\n",
        "        return index_pairs\n",
        "\n",
        "\n",
        "\n",
        "class EchoStateNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, reservoir_size, spectral_radius=0.9, sparsity=0.5, leaky_rate=0.2):\n",
        "        super(EchoStateNetwork, self).__init__()\n",
        "        self.reservoir_size = reservoir_size\n",
        "        self.spectral_radius = spectral_radius\n",
        "        self.leaky_rate = leaky_rate\n",
        "\n",
        "        self.W_in = (torch.rand(reservoir_size, input_dim) - 0.5) * 2 / input_dim\n",
        "\n",
        "        W = torch.rand(reservoir_size, reservoir_size) - 0.5\n",
        "        mask = torch.rand(reservoir_size, reservoir_size) > sparsity\n",
        "        W[mask] = 0\n",
        "\n",
        "        eigenvector = torch.rand(reservoir_size, 1)\n",
        "        for _ in range(50):  # Power iteration\n",
        "            eigenvector = W @ eigenvector\n",
        "            eigenvector = eigenvector / eigenvector.norm()\n",
        "        max_eigenvalue = eigenvector.norm()\n",
        "        self.W = W * (spectral_radius / max_eigenvalue)\n",
        "\n",
        "        self.register_buffer(\"state\", torch.zeros(reservoir_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "        self.state = self.state.to(device)\n",
        "        self.W_in = self.W_in.to(device)\n",
        "        self.W = self.W.to(device)\n",
        "\n",
        "        self.state = (1 - self.leaky_rate) * self.state + self.leaky_rate * torch.tanh(self.W_in @ x + self.W @ self.state)\n",
        "        self.state = self.state / (self.state.norm(dim=0, keepdim=True).clamp(min=1e-6))\n",
        "        return self.state\n",
        "\n",
        "class GraphReinforceAgent(nn.Module):\n",
        "    def __init__(self, input_dimension, output_dimension, esn_reservoir_size=500, hidden_layer_dimension=128, learning_rate=0.0005):\n",
        "        super(GraphReinforceAgent, self).__init__()\n",
        "        self.esn = EchoStateNetwork(input_dim=input_dimension, reservoir_size=esn_reservoir_size)\n",
        "        self.graph_convolution_layer = GCNConv(2, hidden_layer_dimension)\n",
        "        self.hidden_linear_layer = nn.Linear(hidden_layer_dimension + esn_reservoir_size, hidden_layer_dimension)\n",
        "        self.output_layer = nn.Linear(hidden_layer_dimension, output_dimension)\n",
        "        self.normalization_layer = LayerNorm(hidden_layer_dimension)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "        self.scheduler = StepLR(self.optimizer, step_size=100, gamma=0.1)\n",
        "        self.experience_memory = []\n",
        "\n",
        "    def store_transition(self, transition):\n",
        "        self.experience_memory.append(transition)\n",
        "\n",
        "    def forward(self, node_features, edge_index, esn_state):\n",
        "        node_features = F.relu(self.graph_convolution_layer(node_features, edge_index))\n",
        "        node_features = global_add_pool(self.normalization_layer(node_features), torch.LongTensor([0] * 4).to(node_features.device))\n",
        "\n",
        "        node_features = torch.cat((node_features, esn_state), dim=1)\n",
        "        node_features = F.relu(self.hidden_linear_layer(node_features))\n",
        "        output = self.output_layer(node_features)\n",
        "        return F.log_softmax(output, dim=1)\n",
        "\n",
        "    def optimize(self, discount_factor):\n",
        "        cumulative_reward, discounted_rewards, running_discounted_reward = 0, [], 0\n",
        "        for reward, log_prob in reversed(self.experience_memory):\n",
        "            running_discounted_reward = reward + discount_factor * running_discounted_reward\n",
        "            discounted_rewards.append(running_discounted_reward)\n",
        "        discounted_rewards = np.array(discounted_rewards)\n",
        "        rewards_mean, rewards_std_dev = discounted_rewards.mean(), discounted_rewards.std()\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        for reward, log_prob in reversed(self.experience_memory):\n",
        "            cumulative_reward = reward + discount_factor * cumulative_reward\n",
        "            policy_loss = -log_prob * ((cumulative_reward - rewards_mean) / rewards_std_dev)\n",
        "            policy_loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.scheduler.step()\n",
        "        self.experience_memory = []\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    seed = 1234\n",
        "    Utils.set_seed(seed)\n",
        "    env.seed(seed)\n",
        "\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    learning_rate = 0.0005\n",
        "    episodes = 500\n",
        "    gamma = 0.99\n",
        "    print_interval = 10\n",
        "\n",
        "    agent = GraphReinforceAgent(state_dim, action_dim, esn_reservoir_size=500, hidden_layer_dimension=128, learning_rate=learning_rate).to(device)\n",
        "    score_list = []\n",
        "    index_pairs = GraphDataProcessor.construct_index_pairs(state_dim)\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            esn_state = agent.esn(torch.tensor(state, dtype=torch.float).to(device))\n",
        "            graph_data = GraphDataProcessor.create_graph_data(state, index_pairs)\n",
        "            action_probs = agent(graph_data.x.to(device), graph_data.edge_index.to(device), esn_state.unsqueeze(0))\n",
        "\n",
        "\n",
        "            action_distribution = Categorical(torch.exp(action_probs))\n",
        "            action = action_distribution.sample()\n",
        "\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action.item())\n",
        "            agent.store_transition((reward, action_probs[0][action]))\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "\n",
        "        agent.optimize(gamma)\n",
        "        score_list.append(score)\n",
        "\n",
        "        if episode % print_interval == 0 and episode != 0:\n",
        "            avg_score = sum(score_list[-print_interval:]) / print_interval\n",
        "            print(f\"Episode {episode}, Avg Score: {avg_score}\")\n",
        "\n",
        "    env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjfvlNZyxbLJ",
        "outputId": "f97e7c17-7b7f-4932-e8fc-d00ece5a118c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 10, Avg Score: 93.9\n",
            "Episode 20, Avg Score: 476.5\n",
            "Episode 30, Avg Score: 392.0\n",
            "Episode 40, Avg Score: 319.3\n",
            "Episode 50, Avg Score: 346.8\n",
            "Episode 60, Avg Score: 407.7\n",
            "Episode 70, Avg Score: 469.5\n",
            "Episode 80, Avg Score: 454.9\n",
            "Episode 90, Avg Score: 500.0\n",
            "Episode 100, Avg Score: 500.0\n",
            "Episode 110, Avg Score: 480.9\n",
            "Episode 120, Avg Score: 500.0\n",
            "Episode 130, Avg Score: 500.0\n",
            "Episode 140, Avg Score: 500.0\n",
            "Episode 150, Avg Score: 500.0\n",
            "Episode 160, Avg Score: 488.1\n",
            "Episode 170, Avg Score: 500.0\n",
            "Episode 180, Avg Score: 500.0\n",
            "Episode 190, Avg Score: 500.0\n",
            "Episode 200, Avg Score: 500.0\n",
            "Episode 210, Avg Score: 500.0\n",
            "Episode 220, Avg Score: 500.0\n",
            "Episode 230, Avg Score: 500.0\n",
            "Episode 240, Avg Score: 500.0\n",
            "Episode 250, Avg Score: 500.0\n",
            "Episode 260, Avg Score: 500.0\n",
            "Episode 270, Avg Score: 500.0\n",
            "Episode 280, Avg Score: 500.0\n",
            "Episode 290, Avg Score: 500.0\n",
            "Episode 300, Avg Score: 500.0\n",
            "Episode 310, Avg Score: 500.0\n",
            "Episode 320, Avg Score: 497.4\n",
            "Episode 330, Avg Score: 474.4\n",
            "Episode 340, Avg Score: 500.0\n",
            "Episode 350, Avg Score: 500.0\n",
            "Episode 360, Avg Score: 500.0\n",
            "Episode 370, Avg Score: 500.0\n",
            "Episode 380, Avg Score: 500.0\n",
            "Episode 390, Avg Score: 500.0\n",
            "Episode 400, Avg Score: 500.0\n",
            "Episode 410, Avg Score: 500.0\n",
            "Episode 420, Avg Score: 500.0\n",
            "Episode 430, Avg Score: 500.0\n",
            "Episode 440, Avg Score: 479.5\n",
            "Episode 450, Avg Score: 500.0\n",
            "Episode 460, Avg Score: 500.0\n",
            "Episode 470, Avg Score: 500.0\n",
            "Episode 480, Avg Score: 500.0\n",
            "Episode 490, Avg Score: 500.0\n"
          ]
        }
      ]
    }
  ]
}